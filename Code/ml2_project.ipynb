{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing xView Dataset:\n",
    "## Group Project Proposal\n",
    "### Machine Learning II DATS 6203 - 11\n",
    "### Group 2: Jiarong Che, Jiajun Wu, Diana Holcomb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from tqdm import tqdm\n",
    "import geojson\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'74': 'Aircraft Hangar', '71': 'Hut/Tent', '65': 'Cement Mixer', '27': 'Trailer', '32': 'Crane Truck', '72': 'Shed', '28': 'Truck Tractor w/ Flatbed Trailer', '50': 'Yacht', '20': 'Pickup Truck', '94': 'Tower', '89': 'Shipping container lot', '79': 'Construction Site', '38': 'Locomotive', '37': 'Tank car', '34': 'Passenger Car', '57': 'Straddle Carrier', '36': 'Flat Car', '47': 'Fishing Vessel', '23': 'Truck', '45': 'Barge', '83': 'Vehicle Lot', '91': 'Shipping Container', '25': 'Truck Tractor w/ Box Trailer', '33': 'Railway Vehicle', '17': 'Passenger Vehicle', '53': 'Engineering Vehicle', '54': 'Tower crane', '35': 'Cargo/Container Car', '18': 'Small Car', '84': 'Helipad', '73': 'Building', '13': 'Passenger/Cargo Plane', '29': 'Truck Tractor w/ Liquid Tank', '19': 'Bus', '44': 'Tugboat', '61': 'Haul Truck', '49': 'Ferry', '12': 'Small Aircraft', '21': 'Utility Truck', '62': 'Scraper/Tractor', '55': 'Container Crane', '63': 'Front loader/Bulldozer', '64': 'Excavator', '42': 'Sailboat', '93': 'Pylon', '52': 'Oil Tanker', '51': 'Container Ship', '59': 'Mobile Crane', '41': 'Motorboat', '15': 'Helicopter', '24': 'Cargo Truck', '66': 'Ground Grader', '56': 'Reach Stacker', '11': 'Fixed-wing Aircraft', '26': 'Truck Tractor', '77': 'Facility', '60': 'Dump Truck', '76': 'Damaged Building', '40': 'Maritime Vessel', '86': 'Storage Tank'}\n",
      "Num labels: 60\n"
     ]
    }
   ],
   "source": [
    "#Load the class number -> class string label map\n",
    "labels_filepath = './xView_baseline/xview_class_labels.txt'\n",
    "\n",
    "\n",
    "def get_labels(labels_filepath):\n",
    "    labels = {}\n",
    "    with open(labels_filepath) as classfile:\n",
    "        data = classfile.readlines()\n",
    "        for line in data:\n",
    "            if len(line) > 0:\n",
    "                class_num, class_name = line.split(':')\n",
    "                labels[class_num] = class_name.strip()\n",
    "    return labels\n",
    "\n",
    "labels = get_labels(labels_filepath)\n",
    "print(labels)\n",
    "print(\"Num labels: \" + str(len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code in the cell below is the official xView preprocessing code.\n",
    "\n",
    "Found here: https://github.com/DIUx-xView/data_utilities/blob/master/wv_util.py\n",
    "Some mods were made to get_labels to allow a filter to break up test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x,range1=(0,0),range2=(0,0)):\n",
    "    \"\"\"\n",
    "    Linear scaling for a value x\n",
    "    \"\"\"\n",
    "    return range2[0]*(1 - (x-range1[0]) / (range1[1]-range1[0])) + range2[1]*((x-range1[0]) / (range1[1]-range1[0]))\n",
    "\n",
    "\n",
    "def get_image(fname):    \n",
    "    \"\"\"\n",
    "    Get an image from a filepath in ndarray format\n",
    "    \"\"\"\n",
    "    img = torch.from_numpy(np.asarray(Image.open(fname)))\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_labels(fname, filtered_list=None):\n",
    "    \"\"\"\n",
    "    Gets label data from a geojson label file\n",
    "\n",
    "    Args:\n",
    "        fname: file path to an xView geojson label file\n",
    "\n",
    "    Output:\n",
    "        Returns three arrays: coords, chips, and classes corresponding to the\n",
    "            coordinates, file-names, and classes for each ground truth.\n",
    "    \"\"\"\n",
    "    with open(fname) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    if filtered_list != None:\n",
    "        print(\"Pre-filtered geojson feature length: \" +str(len(data['features'])))\n",
    "        # Filter python objects with list comprehensions\n",
    "        data['features'] = [feature for feature in data['features'] if not feature['properties']['image_id'] in filtered_list]\n",
    "        print(\"Post-filtered geojson feature length: \" +str(len(data['features'])))\n",
    "        \n",
    "    coords = np.zeros((len(data['features']),4))\n",
    "    chips = np.zeros((len(data['features'])),dtype=\"object\")\n",
    "    classes = np.zeros((len(data['features'])))\n",
    "    \n",
    "\n",
    "    for i in tqdm(range(len(data['features']))):\n",
    "        if data['features'][i]['properties']['bounds_imcoords'] != []:            \n",
    "            b_id = data['features'][i]['properties']['image_id']\n",
    "            #if (filtered_list == None) or (filtered_list != None and b_id in filtered_list):\n",
    "            val = np.array([int(num) for num in data['features'][i]['properties']['bounds_imcoords'].split(\",\")])\n",
    "            chips[i] = b_id\n",
    "            classes[i] = data['features'][i]['properties']['type_id']\n",
    "            if val.shape[0] != 4:\n",
    "                print(\"Issues at %d!\" % i)\n",
    "            else:\n",
    "                coords[i] = val\n",
    "\n",
    "    return coords, chips, classes\n",
    "\n",
    "\n",
    "def boxes_from_coords(coords):\n",
    "    \"\"\"\n",
    "    Processes a coordinate array from a geojson into (xmin,ymin,xmax,ymax) format\n",
    "\n",
    "    Args:\n",
    "        coords: an array of bounding box coordinates\n",
    "\n",
    "    Output:\n",
    "        Returns an array of shape (N,4) with coordinates in proper format\n",
    "    \"\"\"\n",
    "    nc = np.zeros((coords.shape[0],4))\n",
    "    for ind in range(coords.shape[0]):\n",
    "        x1,x2 = coords[ind,:,0].min(),coords[ind,:,0].max()\n",
    "        y1,y2 = coords[ind,:,1].min(),coords[ind,:,1].max()\n",
    "        nc[ind] = [x1,y1,x2,y2]\n",
    "    return nc\n",
    "\n",
    "\n",
    "def chip_image(img,coords,classes,shape=(300,300)):\n",
    "    \"\"\"\n",
    "    Chip an image and get relative coordinates and classes.  Bounding boxes that pass into\n",
    "        multiple chips are clipped: each portion that is in a chip is labeled. For example,\n",
    "        half a building will be labeled if it is cut off in a chip. If there are no boxes,\n",
    "        the boxes array will be [[0,0,0,0]] and classes [0].\n",
    "        Note: This chip_image method is only tested on xView data-- there are some image manipulations that can mess up different images.\n",
    "\n",
    "    Args:\n",
    "        img: the image to be chipped in array format\n",
    "        coords: an (N,4) array of bounding box coordinates for that image\n",
    "        classes: an (N,1) array of classes for each bounding box\n",
    "        shape: an (W,H) tuple indicating width and height of chips\n",
    "\n",
    "    Output:\n",
    "        An image array of shape (M,W,H,C), where M is the number of chips,\n",
    "        W and H are the dimensions of the image, and C is the number of color\n",
    "        channels.  Also returns boxes and classes dictionaries for each corresponding chip.\n",
    "    \"\"\"\n",
    "\n",
    "    height,width,channels = img.shape\n",
    "    wn,hn = shape\n",
    "    \n",
    "    w_num,h_num = (int(width/wn),int(height/hn))\n",
    "    images = np.zeros((w_num*h_num,hn,wn,channels))\n",
    "    total_boxes = {}\n",
    "    total_classes = {}\n",
    "    \n",
    "    k = 0\n",
    "    for i in range(w_num):\n",
    "        for j in range(h_num):\n",
    "            x = np.logical_or( np.logical_and((coords[:,0]<((i+1)*wn)),(coords[:,0]>(i*wn))),\n",
    "                               np.logical_and((coords[:,2]<((i+1)*wn)),(coords[:,2]>(i*wn))))\n",
    "            out = coords[x]\n",
    "            y = np.logical_or( np.logical_and((out[:,1]<((j+1)*hn)),(out[:,1]>(j*hn))),\n",
    "                               np.logical_and((out[:,3]<((j+1)*hn)),(out[:,3]>(j*hn))))\n",
    "            outn = out[y]\n",
    "            out = np.transpose(np.vstack((np.clip(outn[:,0]-(wn*i),0,wn),\n",
    "                                          np.clip(outn[:,1]-(hn*j),0,hn),\n",
    "                                          np.clip(outn[:,2]-(wn*i),0,wn),\n",
    "                                          np.clip(outn[:,3]-(hn*j),0,hn))))\n",
    "            box_classes = classes[x][y]\n",
    "            \n",
    "            if out.shape[0] != 0:\n",
    "                total_boxes[k] = out\n",
    "                total_classes[k] = box_classes\n",
    "            else:\n",
    "                total_boxes[k] = np.array([[0,0,0,0]])\n",
    "                total_classes[k] = np.array([0])\n",
    "            \n",
    "            chip = img[hn*j:hn*(j+1),wn*i:wn*(i+1),:channels]\n",
    "            images[k]=chip\n",
    "            \n",
    "            k = k + 1\n",
    "    \n",
    "    return images.astype(np.uint8),total_boxes,total_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing \n",
    "\n",
    "First we must be able to break the image up into chips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(original_image_loc, groundtruth_filepath, chip_size, limit=None):\n",
    "    # first process the geojson\n",
    "    coords, chips, classes = get_labels(groundtruth_filepath)\n",
    "    chip_bytes = []\n",
    "    chip_coords = []\n",
    "    chip_labels = []\n",
    "    chip_orig_image_names = []\n",
    "    image_names = [f for f in os.listdir(original_image_loc) if os.path.isfile(os.path.join(original_image_loc, f))]\n",
    "    print(\"Number of images to preprocess: \" + str(len(image_names)))\n",
    "    count = 0;\n",
    "    for _file in image_names:\n",
    "        if limit != None and count < limit:\n",
    "            c_img, c_box, c_cls = chip_image(get_image(original_image_loc+_file), coords, classes, shape=(chip_size,chip_size))\n",
    "            for i in range(len(c_img)):\n",
    "                chip_bytes.append(c_img[i])\n",
    "                chip_coords.append(c_box[i])\n",
    "                chip_labels.append(c_cls[i])\n",
    "                chip_orig_image_names.append(_file)\n",
    "            print(\".\", end =\"\") \n",
    "        count = count + 1\n",
    "        \n",
    "    print(\"Done with preprocessing.\")\n",
    "    return chip_bytes, chip_coords, chip_labels, chip_orig_image_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom PyTorch Dataset to load the xview files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTransformer(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, base_dataset, transform):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.base_dataset[index]\n",
    "        return self.transform(img), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "class xViewDataset(Dataset):\n",
    "    # input is image, target is annotation\n",
    "    def __init__(self, image_arr, coord_arr, label_arr,\n",
    "                 transform=None, target_transform=None): \n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.images = image_arr\n",
    "        print(\"Len of img array: \" + str(len(self.images)))\n",
    "        # filter the giant geojso data into just the images included in the root folder\n",
    "        self.coords = coord_arr\n",
    "        self.labels = label_arr  \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im, gt, h, w = self.pull_item(index)\n",
    "\n",
    "        return im, gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) #self.images.shape[0]\n",
    "\n",
    "    def pull_item(self, index):      \n",
    "        img = self.images[index]\n",
    "        height, width, channels = img.shape\n",
    "        \n",
    "        target = self.labels[index]\n",
    "        if self.target_transform is not None:\n",
    "            # convert the bbox and label into one\n",
    "            target = self.target_transform(self.coords[index], np.array(self.labels[index], ndmin=2).T, height, width)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            target = np.array(target)\n",
    "            img, boxes, labels = self.transform(img, target[:, :4], target[:, 4])\n",
    "            target = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
    "        \n",
    "        return torch.from_numpy(img).permute(2, 0, 1), target, height, width\n",
    "        # return torch.from_numpy(img), target, height, width\n",
    "\n",
    "    def pull_image(self, index):\n",
    "        '''Returns the original image object at index in PIL form\n",
    "        Note: not using self.__getitem__(), as any transformations passed in\n",
    "        could mess up this functionality.\n",
    "        Argument:\n",
    "            index (int): index of img to show\n",
    "        Return:\n",
    "            PIL img\n",
    "        '''\n",
    "        image = self.images[index]\n",
    "        return PIL.Image.fromarray(image, mode=\"RGB\")\n",
    "\n",
    "    def pull_anno(self, index):\n",
    "        '''Returns the original annotation of image at index\n",
    "        Note: not using self.__getitem__(), as any transformations passed in\n",
    "        could mess up this functionality.\n",
    "        Argument:\n",
    "            index (int): index of img to get annotation of\n",
    "        Return:\n",
    "            list:  [img_id, [(label, bbox coords),...]]\n",
    "                eg: ('001718', [('dog', (96, 13, 438, 332))])\n",
    "        '''\n",
    "        img_id = str(index)\n",
    "        gt = self.target_transform(self.boxes[index], self.labels[index], 1, 1)\n",
    "\n",
    "        return img_id, gt\n",
    "\n",
    "    def pull_tensor(self, index):\n",
    "        '''Returns the original image at an index in tensor form\n",
    "        Note: not using self.__getitem__(), as any transformations passed in\n",
    "        could mess up this functionality.\n",
    "        Argument:\n",
    "            index (int): index of img to show\n",
    "        Return:\n",
    "            tensorized version of img, squeezed\n",
    "        '''\n",
    "        return torch.Tensor(self.pull_image(index)).unsqueeze_(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_size = 300\n",
    "in_channels = 3\n",
    "input_size = chip_size**2\n",
    "num_classes = len(labels)\n",
    "num_epochs = 100\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "train_path = './xView/train_images/'\n",
    "test_path = './xView/val_images/'\n",
    "geojson_path = './xView/xView_train.geojson'\n",
    "kernel_size = 5\n",
    "conv_size = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose whether to preprocess the raw data, or load the pre-saved pickle files:\n",
    "\n",
    "* train_chip_bytes = 'train_chip_bytes.pkl' \n",
    "  * an array of the bytes of all the chipped images. \n",
    "  * Size: (35858, 300, 300, 3)\n",
    "* train_chip_coords = 'train_chip_coords.pkl'\n",
    "  * an array of bounding boxes per each chipped image\n",
    "  * Size: (35858,)\n",
    "* train_chip_labels = 'train_chip_labels.pkl'\n",
    "  * an array of labels corresponding to the bounding box per each chipped image\n",
    "  * Size: (35858,)\n",
    "* train_chip_image_names = 'train_chip_image_names.pkl'\n",
    "  * an array of original image names per each chipped image\n",
    "  * Size: (35858,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601937/601937 [00:03<00:00, 197156.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images to preprocess: 379\n",
      "....................Done with preprocessing.\n"
     ]
    }
   ],
   "source": [
    "load_from_pkl = False\n",
    "\n",
    "if (load_from_pkl and os.path.isfile('train_chip_bytes.pkl') and os.path.isfile('train_chip_coords.pkl') and os.path.isfile('train_chip_labels.pkl')):\n",
    "    train_chip_bytes = torch.load('train_chip_bytes.pkl')\n",
    "    train_chip_coords = torch.load('train_chip_coords.pkl')\n",
    "    train_chip_labels = torch.load('train_chip_labels.pkl')\n",
    "    train_chip_image_names = torch.load('train_chip_image_names.pkl')\n",
    "    \n",
    "else :\n",
    "    # Here either do preprocessing, or load the pickle files\n",
    "    train_chip_bytes, train_chip_coords, train_chip_labels, train_chip_image_names = preprocess_dataset(train_path, geojson_path, chip_size, limit=20)\n",
    "\n",
    "# print(\"train_chip_bytes: \")\n",
    "# print(np.shape(train_chip_bytes)) \n",
    "\n",
    "# print(\"train_chip_coords: \")\n",
    "# print(np.shape(train_chip_coords))\n",
    "\n",
    "# print(\"train_chip_labels: \")\n",
    "# print(np.shape(train_chip_labels))\n",
    "\n",
    "# print(\"train_chip_image_names: \")\n",
    "# print(np.shape(train_chip_image_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save off the outputs so we can share and reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide if you want to save off the preprocessed data - it is quite time consuming\n",
    "do_save_pkls = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_save_pkls:\n",
    "    torch.save(train_chip_bytes, 'train_chip_bytes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_save_pkls:\n",
    "    torch.save(train_chip_coords, 'train_chip_coords.pkl') # this seems to take a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_save_pkls:\n",
    "    torch.save(train_chip_labels, 'train_chip_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_save_pkls:\n",
    "    torch.save(train_chip_image_names, 'train_chip_image_names.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create the Train Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1929, 300, 300, 3)\n",
      "Len of img array: 1929\n"
     ]
    }
   ],
   "source": [
    "def annotation_collate(batch):\n",
    "    targets = []\n",
    "    images = []\n",
    "    pad_size = 0\n",
    "    longest_target_idx = 0\n",
    "    for i, item in enumerate(batch):\n",
    "        images.append(item[0])\n",
    "        if len(item[1]) > longest_target_idx:\n",
    "            longest_target_idx = len(item[1])        \n",
    "        targets.append(np.array(item[1])) #torch.FloatTensor(item[1]))\n",
    "        \n",
    "    for j, t in enumerate(targets):\n",
    "        amount_to_pad = longest_target_idx - len(t)\n",
    "        zeros = np.zeros(amount_to_pad)\n",
    "        targets[j] = np.append(t, zeros)\n",
    "\n",
    "    return torch.stack(images, 0), np.array(targets) #torch.stack(images,0) # targets\n",
    "\n",
    "#transforms.ToTensor() converts our PILImage to a tensor of shape (C x H x W) in the range [0,1]\n",
    "#transforms.Normalize(mean,std) normalizes a tensor to a (mean, std) for (R, G, B)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "print(np.shape(train_chip_bytes))\n",
    "\n",
    "# training\n",
    "train_dataset = xViewDataset(image_arr=train_chip_bytes, coord_arr=train_chip_coords, label_arr=train_chip_labels, transform=None)\n",
    "#train_dataset = DatasetTransformer(train_dataset, transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size,shuffle=True,collate_fn=annotation_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/ATen/native/cuda/SoftMax.cu:572",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-748229bfc38d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/ATen/native/cuda/SoftMax.cu:572"
     ]
    }
   ],
   "source": [
    "# CNN Model (2 conv layer)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, conv_size, kernel_size=kernel_size, padding=2),\n",
    "            nn.BatchNorm2d(conv_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(conv_size, conv_size*2, kernel_size=kernel_size, padding=2),\n",
    "            nn.BatchNorm2d(conv_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(3375000, num_classes) #conv_size**2 * in_channels * (conv_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "# -----------------------------------------------------------------------------------\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "cnn = CNN()\n",
    "cnn.cuda()\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "#if cuda:\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "# -----------------------------------------------------------------------------------\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.to(device, dtype=torch.float32)).cuda()\n",
    "        labels = Variable(torch.from_numpy(labels)).cuda()\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)       \n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "test_dataset = xViewDataset(image_folder=test_path, groundtruth_filename=geojson_path, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# Test the Model\n",
    "cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images).cuda()\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "# -----------------------------------------------------------------------------------\n",
    "print('Test Accuracy of the model on the test images: %d %%' % (100 * correct / total))\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Save the Trained Model\n",
    "torch.save(cnn.state_dict(), 'cnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
